{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "\n",
    "# path_to_file = \"shakespeare.txt\" ---> Do note that the shakespear is just an example used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                  Opening the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = open(path_to_file,'r').read() ---> Reading the text file, r: Open for reading\n",
    "\n",
    "# Reading the file\n",
    "\n",
    "# text[:500] ---> Reading the first 500 characters in it\n",
    "\n",
    "# Printing the texts\n",
    "\n",
    "# print(text[4500:4800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                               Understanding Unique Characters inside the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(set(text)) ---> Displays all the unique characters\n",
    "\n",
    "# vocab = sorted(set(text)) ---> Assigning the unique characters as vocab(vocabulary)\n",
    "\n",
    "# len(vocab) ---> Displays the number of unique characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                              Assigning a number for every Characters(Enumeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in vocab:\n",
    "#    print (pair)\n",
    "\n",
    "# [This will essentially output tuples consisting of a number and character]\n",
    "\n",
    "# *Tuples:Tuples are sequences, just like lists. The differences between tuples and lists are, the tuples cannot be changed \n",
    "#         unlike lists and tuples use parentheses*\n",
    "\n",
    "\n",
    "# Generating a Dictionary of the Vocab\n",
    "\n",
    "# char_to_ind = {char:ind for ind, char in enumerate(vocab)} ---> This will enumerate all the characters\n",
    "\n",
    "# char_to_ind['H'] ---> Displays the numeric index for the letter 'H'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                     Encoding the Entire Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_text = np.array([char_to_ind[c] for c in text]) ---> This step will encode all the texts into integers\n",
    "\n",
    "# encoded_text.shape ---> Displays how many characters we have in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                           Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: We need to ensure that the training sequence are long enough to pick up the general structure of the text\n",
    "#         Eg; Shakespeare's tends to rhyme so maybe we can use 3 lines to understand the structure\n",
    "\n",
    "#         lines = '''[Insert the lines here]''' ---> Do note that we are using triple quotes here as we are using multi-line\n",
    "#        len(lines) ---> Displays the total characters for the variable lines.\n",
    "\n",
    "\n",
    "# Step 2: Once we established the lines we want as batches and the amount of charaters we take in Eg; 120 for example below.\n",
    "#         We can calculate the total number of sequences\n",
    "\n",
    "#         seq_len = 120\n",
    "#         total_num_seq = len(text) // (seq_len+1) ---> 2 forward slashes for classic division two round it off. \n",
    "#                                                  ---> +1 for zero index. Cause it starts from 0 instead of 1 in the index.\n",
    "\n",
    "\n",
    "# Step 3: Creating training Sequences using special dataset object from Tensorflow\n",
    "\n",
    "#        char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "#        type(char_dataset) ---> Shows that it is a special Tensor Slice Dataset\n",
    "#        char_dataset.[Press Tab] ---> Displays a bunch of method calls\n",
    "#        char_dataset.batch ---> Converts individual character calls into sequences we can feed in as a batch\n",
    "\n",
    "#        sequences = char_dataset.batch(seq_len+1, drop_remainder = True)\n",
    "#        drop_remainder: Cos if we do a normal divide of the len(text) with seq_len, we will end up with remainders mah.\n",
    "#                        So we just remove them to make it an integer. Won't affect much one.\n",
    "\n",
    "\n",
    "# Step 4: Creating Sequence Targets\n",
    "\n",
    "# [We'll grab the input text sequence, assign the target text sequence as the input text sequence\n",
    "# shifted by 1 step forward and group them together as a tuple]\n",
    "\n",
    "#  def create_seq_targets(seq):\n",
    "#       input_txt = seq[:-1]     || 'Hello my nam'\n",
    "#       target_txt = seq[1:]    || 'ello my name'\n",
    "#       return input_txt, target_txt\n",
    "\n",
    "\n",
    "# Step 5: Mapping to all the sequences\n",
    "\n",
    "#  dataset = sequences.map(create_seq_targets)\n",
    "\n",
    "# =============================================Just for testing===========================================================\n",
    "#  for input_txt, target_txt in dataset.take(1):\n",
    "#     print(input_txt.numpy())                       ---> Displays the index of characters\n",
    "#     print(\"\".join(ind_to_char[input_txt.numpy()])) ---> Displays the lines of characters and characters itself\n",
    "#     print('\\n')             \n",
    "#     print(target_txt.numpy())\n",
    "#     print(\"\".join(ind_to_char[target_txt.numpy()]))\n",
    "#==========================================================================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Selecting the batch size\n",
    "\n",
    "# batch_size = 128\n",
    "\n",
    "\n",
    "# Step 2: We need to shuffle the texts so that the model does not learn any particular ordering of text\n",
    "#         We also need to ensure that everthing is not shuffled in memory. So, we only take batches at a time.\n",
    "\n",
    "# buffer_size = 10000 ---> Just take in 10000 of the elements and shuffle them around. If we don't specify, it will attempt to\n",
    "#                          read the entire dataset in memory which may cause error.\n",
    "# dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)\n",
    "\n",
    "# dataset ---> Outputs the shape which is 128 rows as specified and the sequence as columns.\n",
    "#         ---> Essentially saying we are gonna feed 128 batches of 120 characters long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Getting the Vocab Size\n",
    "\n",
    "# vocab_size = len(vocab) ---> Assigns the number of unique characters to variable vocab_size to be used as parameter in model\n",
    "# embed_dim = 64 ---> Something you can play around with. Do note that you shouldn't go too large on this\n",
    "#                ---> Choose a number which is the same scale as the actual vocabulary size.\n",
    "# rnn_neurons = 1026 ---> Since we are creating a simple model with a single layer,  we will have lots of neurons\n",
    "#                    ---> This is also something you can play with like using LSTM layers or GRU or dropout etc\n",
    "\n",
    "\n",
    "# Step 2: Creating a loss function & importing\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "# from tensorflow.keras.losses import sparse_categorical_crossentropy ---> As our layers are one-hot encoded\n",
    "\n",
    "# def sparse_cat_loss(y_true, y_pred):\n",
    "#    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "# [The reason why we need to create this function is because the sparse_cat_crossentropy's logits is by default false.\n",
    "# However since our data is one-hot encoded, we can't just pass in this function by default.]\n",
    "\n",
    "\n",
    "# Step 3: Creating the model function [Do note that we are using a function to create the model]\n",
    "\n",
    "# def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, embed_dim, batch_input_shape = [batch_size, None]))\n",
    "#     model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "#     model.add(Dense(vocab_size))\n",
    "\n",
    "#     model.compile('adam', loss=sparse_cat_loss)\n",
    "#     loss=sparse_cat_loss: As we are not able to pass in the original sparse_categorical_crossentropy with the amended parameter,\n",
    "#                       we are passing in the function instead\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Creating the model\n",
    "\n",
    "# model = create_model(vocab_size = vocab_size, embed_dim = embed_dim, rnn_neurons = rnn_neurons, batch_size = batch_size)\n",
    "# [Do note that all the parameters were already specified in the prev steps so we just pass it in as arguments]\n",
    "\n",
    "# model.summary() ---> Display the model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30 ---> 30 is the minimum to get realistic results\n",
    "# model.fit(dataset, epochs=epochs) ---> It will take awhile to run finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model (As per the Tutorial to not waste time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size=1) ---> Creating the model with the specified neurons etc.\n",
    "\n",
    "# model.load_weights('[Insert the model file here with the .h5]') ---> We are just loading the weights\n",
    "\n",
    "# model.build(tf.TensorShape([1,None])) --->Building the model with the shape and size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Basic idea of the function below is to take in a seed text, format it so that it is in the correct shape for a network and\n",
    "# loop the sequence as we keep adding our own predicted characters]\n",
    "# The function below will take in the model, \n",
    "# start_seed: Starting seed which is like u type in some name or words and the texts will start from. A string for users to pass\n",
    "# gen_size: How many characters you want to generate\n",
    "# temp: An argument that you can play around with. The higher it is, the more surprising the results\n",
    "\n",
    "# def generate_text(model, start_seed, gen_size=500, temp=1.0):\n",
    "\n",
    "#     num_generate = gen_size\n",
    "#     input_eval = [char_to_ind[s] for s in start_seed] ---> Transforms every character from the start seed into index\n",
    "#     input_eval = tf.expand_dims(input_eval,0) ---> Expands the dimensions so that it is in the correct shape\n",
    "#     text_generated = [] ---> An empty list at first to hold our generated text\n",
    "#     temperature = temp\n",
    "#     model.reset_states()\n",
    "    \n",
    "#     for i in range(num_generate):\n",
    "        \n",
    "#         predictions = model(input_eval) ---> Generate some predictions\n",
    "#         predictions = tf.squeeze(predictions,0) ---> The opposite of expand_dims as we'll be removing the batch shape\n",
    "#         predictions = predictions/temperature\n",
    "#         predicted_id=tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "#         input_eval = tf.expand_dims([predicted_id],0)\n",
    "#         text_generated.append(ind_to_char[predicted_id]) ---> Converting the index to character and appending it\n",
    "        \n",
    "#         return(start_seed+\"\".join(text_generated))\n",
    "\n",
    "\n",
    "# print(generate_text(model,\"JULIET\", gen_size=1000))\n",
    "#'JULIET': This is the start_seed we are loading in which the model will generated texts from then on\n",
    "# We never put in temp as it is set as 1.0 by default. You can specify it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================================================\n",
    "# ANOTHER WAY TO COPY AND PASTE WITH THE EXPLANATIONS\n",
    "#===========================================================================================================================\n",
    "\n",
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    " \n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "\n",
    "  for i in range(num_generate):\n",
    "\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "\n",
    "  return (start_seed + ''.join(text_generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
