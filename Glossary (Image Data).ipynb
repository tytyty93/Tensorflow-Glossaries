{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Loading MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Basic Autoendcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# [We will keep dividing the 784 into half since it will be too much to ask to suddenly decrease from 784 to 25 dimensions. \n",
    "# So we slowly cut it in half till 25]\n",
    "\n",
    "# encoder = Sequential()\n",
    "# encoder.add(Flatten(input_shape = [28,28]))\n",
    "# encoder.add(Dense(400,activation=\"relu\"))---> 784/2 = 392(~400)\n",
    "# encoder.add(Dense(200,activation=\"relu\"))\n",
    "# encoder.add(Dense(100,activation=\"relu\"))\n",
    "# encoder.add(Dense(50,activation=\"relu\"))\n",
    "# encoder.add(Dense(25,activation=\"relu\"))\n",
    "\n",
    "# [We start off with 50 as 25 is the middle portion and expand back up. We included the input shape to show the connection\n",
    "#  between the encoder and decoder.]\n",
    "\n",
    "# decoder = Sequential()\n",
    "# decoder.add(Dense(50,input_shape=[25],activation='relu'))\n",
    "# decoder.add(Dense(100,activation='relu'))\n",
    "# decoder.add(Dense(200,activation='relu'))\n",
    "# decoder.add(Dense(400,activation='relu'))\n",
    "# decoder.add(Dense(728, activation=\"sigmoid\"))\n",
    "# decoder.add(Reshape([28, 28])) ---> Must change this to sigmoid. We are performing a binary crossentropy loss\n",
    "#                                     at the end of auto encoder to check does the final output image match the\n",
    "#                                     true image.\n",
    "# decoder.add(Reshape([28,28]))---> As the input was 28 by 28 initially\n",
    "\n",
    "\n",
    "# autoencoder = Sequential([encoder, decoder])\n",
    "# autoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\n",
    "# binary crossentropy: As the autoencoder doesn't care how many distinct types of numbers are there as it only cares about \n",
    "# the image output\n",
    "# optimizer: Can play around with it. Can slow down learning rate by giving a smaller value or increase the learning rate by\n",
    "#            giving it a larger number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder.fit(X_train, X_train, epochs=5, validation_data=[X_test,X_test])\n",
    "# [We don't have to pass in train test split as it is irrelevant since it is an auto encoder (semi-supervised learning)]\n",
    "# [Basically the first X_train is image number 1 equal to image number 1 from X_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passed_images = autoencoder.predict(X_test[:10]) ---> Passing 10 images that the auto encoder was not trained on and asking\n",
    "#                                                      the auto encoder to 'predict' by attempting to reconstruct the image \n",
    "#                                                      after reducing the dimensionality down to 25 neurons\n",
    "\n",
    "# n= 0\n",
    "# print(\"Original Image\")\n",
    "# plt.imshow(X_test[n])\n",
    "# plt.show()\n",
    "# print('Attempted Reconstruction (after autoencoder)')\n",
    "# plt.imshow(passed_images[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Reduction (Creating the Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add noise to our image (In order to reduce it)\n",
    "\n",
    "# from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "# sample = GaussianNoise(0.2) ---> It takes in ONLY standard deviation. The larger the number, the more 'noise'\n",
    "\n",
    "# noisey = sample(X_test[:10], training = True) ---> Grabbing the first 10 images as a batch and makes sure that it trains and\n",
    "#                                                    adds noise\n",
    "\n",
    "\n",
    "# [The codes below compares the original picture with the 'noisy' one]\n",
    "# n=0\n",
    "# print(\"ORIGINAL\")\n",
    "# plt.imshow(X_test[n])\n",
    "# plt.show() ---> Just removed the matplotlib description on top (Optional)\n",
    "# print(\"NOISE VERSION\")\n",
    "# plt.imshow(noisey[n])\n",
    "# plt.show()                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Reduction (Creating the AutoEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# tf.random.set_seed(101) ---> Just to ensure that the same noise created is the same as the one in tutorial video\n",
    "# np.random.seed(101)     --->\n",
    "\n",
    "# encoder = Sequential()\n",
    "# encoder.add(Flatten(input_shape = [28,28]))\n",
    "# encoder.add(GaussianNoise(0.2)) ---> We will add this additional layer into the encoder. You can play around with the number\n",
    "# encoder.add(Dense(400,activation=\"relu\"))---> 784/2 = 392(~400)\n",
    "# encoder.add(Dense(200,activation=\"relu\"))\n",
    "# encoder.add(Dense(100,activation=\"relu\"))\n",
    "# encoder.add(Dense(50,activation=\"relu\"))\n",
    "# encoder.add(Dense(25,activation=\"relu\"))\n",
    "\n",
    "\n",
    "# decoder = Sequential()\n",
    "# decoder.add(Dense(50,input_shape=[25],activation='relu'))\n",
    "# decoder.add(Dense(100,activation='relu'))\n",
    "# decoder.add(Dense(200,activation='relu'))\n",
    "# decoder.add(Dense(400,activation='relu'))\n",
    "# decoder.add(Dense(728, activation=\"sigmoid\"))\n",
    "# decoder.add(Reshape([28, 28])) ---> Must change this to sigmoid. We are performing a binary crossentropy loss\n",
    "#                                     at the end of auto encoder to check does the final output image match the\n",
    "#                                     true image.\n",
    "\n",
    "\n",
    "# noise_remover = Sequential([encoder, decoder])\n",
    "# noise_remover.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# [You can play around with the optimizer eg; Using SGD and keep track of the accuracy]\n",
    "\n",
    "# noise_remover.fit(X_train,X_train,epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten_noisey_images = sample(X_test[:10], training =True) ---> Creating 10 noisey images\n",
    "\n",
    "# denoised = noise_remover(ten_noisey_images) ---> We then pass it through the noise remover autoencoder\n",
    "\n",
    "# n=0\n",
    "# print(\"ORIGINAL IMAGE\")\n",
    "# plt.imshow(X_test[n])\n",
    "# plt.show()\n",
    "# print(\"NOISE ADDED IMAGE\")\n",
    "# plt.imshow(ten_noisey_images[n])\n",
    "# plt.show()\n",
    "# print(\"AFTER NOISE REMOVAL FROM AUTO ENCODER\")\n",
    "# plt.imshow(denoised[n])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
