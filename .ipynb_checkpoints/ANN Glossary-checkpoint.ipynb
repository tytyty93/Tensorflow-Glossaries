{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                            Multi-Class Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Exclusive Classes\n",
    "\n",
    "#Eg; A photo with multiple tags to it. Eg; Beach, Family, Holiday etc\n",
    "\n",
    "# The model can output multiple classes(Classification). \n",
    "# With the use of sigmoid function which gives out the probability between 0 and 1, if the model predicted the picture to be \n",
    "# 0.6 beach and 0.8 vacation, it will output both classes. [If you specify to include all above 0.5]\n",
    "\n",
    "# In essence, one data fed into a function can output multiple classes(results/answers/classifcation)\n",
    "\n",
    "#  O(Picture) ---> O (Passes through function)--->O (0.6 Beach)\n",
    "#                                             --->O (0.8 Vacation)\n",
    "#                                             --->O (0.2 Cruise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutually Exclusive Classes\n",
    "\n",
    "#Eg; Photo categorised as grayscale(black and white) or full color.\n",
    "#    CANNOT BE BOTH AT THE SAME TIME\n",
    "\n",
    "# Each data point can only have a single class(classification). Eg; Can only be Red, Blue or Green\n",
    "# Using the Softmax Activation Function, it calculates the PROBABILITIES of each class OVER POSSIBLE target classes.\n",
    "# The range is 0 to 1 and the sum of all probabilities will be equal to 1.\n",
    "# The model returns the probabilities of each class. Eg; 0.2 Green, 0.7 Blue, 0.1 Red [Note that they add up to 1]\n",
    "# The model will only select Blue as it has the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                             Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It must be an average so it can output a single value. \n",
    "# Because If you output multiple value, how to determine the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using y:True Value, a:Neuron's prediction\n",
    "# a = σ(Z) where Z = Weight*x + bias  & σ is the function used eg;Sigmoid, ReLU(Rectified Linear Unit)\n",
    "# We simply calculate the difference between Real Values: y(x) against Predicted Values: a(x)\n",
    "\n",
    "# After getting the result, we will square it because \n",
    "#(1) We are calculating the mean. So if there is a negative and positives, \n",
    "#    the result could be 0 and it is not showing the actual difference(cost) \n",
    "#(2) To punish large errors which could go undetected when compared against other not so large errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we figure out what is the best weight which leads us to the lowest cost given that there are so many layers with \n",
    "# so many weights involved?\n",
    "\n",
    "# In order to figure the weights to lead us to the lowest cost, we use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                         Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine a graph with a U shaped line, with Weight on X axis and Cost on Y axis.\n",
    "# We will want the weight to fall under the bottom of U where the Cost is the lowest.\n",
    "# We will then take 'Steps' from the tip of the U which measures the gradient and slowly work out way till\n",
    "# the lowest gradient close to 0\n",
    "# But how big of a step should we take? A small step will take a lot of time measuring and a large step could risk over-stepping\n",
    "\n",
    "# This is where Adaptive Gradient Descent comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                    Adaptive Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Adaptive Gradient Descent, we start with larger steps which gets smaller when the slope gets closer to 0.\n",
    "# ADAM: A Method for Stochastic Optimization will be used to search for these minimums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                         Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = df[['Feature1','Feature2']].values   ----> Returns the columns as arrays which will be required\n",
    "#                                          ----> Do note that the features are the columns you want the model to train\n",
    "#                                               to identify Eg; The price of the house\n",
    "#                                           ----> It is 2 brackets as it is a 2D Array and capital X to denote it\n",
    "\n",
    "# Alternatively\n",
    "\n",
    "# X = df.drop('price', axis=1).values ---> Works the same way as the above, more convenient as you dont have to\n",
    "#                                          state all the features.\n",
    "\n",
    "\n",
    "\n",
    "# y = df['Column you want to predict'].values  ----> Using the example of the house, it could be the price of the house\n",
    "\n",
    "\n",
    "# Alternatively ver:2 [This one is get from the youtube video. Once you pop ah y straight away get the label column\n",
    "#                       and is removed from df at the same time. After that you just label it as X.]\n",
    "\n",
    "# y = df.pop('Label column')\n",
    "# X = df\n",
    "\n",
    "\n",
    "\n",
    "# Input the code: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#    random_state ----> It will split the samples into train and test randomly. \n",
    "#    random_state=42 ----> It will always split them into the same sets at random. Eg; Like to follow the lecture's figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                         MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are working with Weights and Biases, if we have large values in the Feature columns, it could cause errors with weights.\n",
    "# To avoid that we normalise/scale it.\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_train) ----> Calculates the parameters it needs to perform the actual scaling later on\n",
    "#                     ----> Calculates the Standard Deviation Min and Max\n",
    "#                     ----> We only run on training set and not test set as well is to prevent Data Leakage \n",
    "#                           We don't want to assume that we have prior information of the test set\n",
    "#                     ----> If you run on the test set, it means that we have the std and min max of it which is not what we want\n",
    "\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_train = scaler.fit_transform(X_train) ---> Does the fitting and transform in a single line\n",
    "\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                   Creating Model with Keras Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# model = Sequential()                 ----> Dense: How many layers of Neurons in a 'column'\n",
    "# model.add(Dense(4,activation='relu'))       Activation: Which activation method you want to use\n",
    "# model.add(Dense(2,activation='relu'))       The copy and pasting: It is to determine how many 'columns' you have which this case \n",
    "# model.add(Dense(1,activation='relu'))       there's three\n",
    "#                                          [We usually determine the Dense number based on the number of features]\n",
    "\n",
    "# model.add(Dense(1)) ----> There's only 1 layer in the final layer to output the results Eg; Predicted Price\n",
    "\n",
    "# model.add(Dense(1,activation='sigmoid')) ----> For Binary Classification, we want the last activation to be Sigmoid\n",
    "\n",
    "# model.compile(optimizer = 'rmsprop', loss = 'mse') ----> Since this is a Regression model[Predicting the price of house].\n",
    "#                                                          We will use loss = 'mse'\n",
    "\n",
    "# model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])   ----> For predicting binary problems[Eg; Yes or No]\n",
    "# model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics=['accuracy']) ----> For predicting classification problems\n",
    "#                                                                                                     Eg; 0.6 Beach 0.4 Vacation photo\n",
    "\n",
    "# model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test),batch_size=128, epochs=250) \n",
    "\n",
    "# Insert the X_train which is the data you want to train & y_train to y which is the output\n",
    "# Epochs refers to one cycle through the full training dataset. So for this case, it is running 250 cycles through the training set\n",
    "# validation_data ---> After each epoch, it will quickly run on the test data and check the loss.\n",
    "#                      It will not affect the weights and bias of the model. They are only adjusted based on the training set.\n",
    "#                      Only to check on the data. So as to identify if there is overfitting\n",
    "# batch_size ---> For large data sets, we can but not must feed it in batches. The smaller the batch, the longer it will run \n",
    "#                 BUT less liklely will overfit which is good!             \n",
    "#            ---> The number is usually in powers of 2 Eg; 32, 64, 128, 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                              Plotting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df = pd.DataFrame(model.history.history) ---> It will display the loss on the training set & validation set if included\n",
    "# loss_df.plot()                            ----> This will display a graph to evaluate how much losses are decreased over time\n",
    "\n",
    "\n",
    "# Alternatively\n",
    "\n",
    "# plt.plot(r.history['loss'], label = 'loss')\n",
    "# plt.plot(r.history['val_loss'], label = 'vaL_loss')  # Do note that the r in this case is the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                             Callbacks (Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisite\n",
    "\n",
    "# Perform these steps again from above before Early Stopping: model = Sequential()\n",
    "#                                                             model = add()\n",
    "#                                                             model.compile()\n",
    "\n",
    "\n",
    "#Importing Early stopping\n",
    "\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# help(EarlyStopping) ---> To read up on the guide\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='val_loss', mode = 'min')\n",
    "\n",
    "# monitor: Refers to the Quantity being monitored. For this case above, it is the val_loss[Do note taht val_loss is system defined term]\n",
    "\n",
    "# patience: Number of epochs to continue running even after there's no improvements.\n",
    "\n",
    "# mode: 'min' = Training stops when quantity monitored(val_loss in this case) stopped decreasing.\n",
    "#       'max' = Training stops when quantity monitored(val_loss in this case) stopped increasing.\n",
    "#       'auto' = Direction is inferred from the name of monitored quantity\n",
    "\n",
    "# Tip: val_loss usually pairs with mode = 'min'. If you are looking for accuracy, go for mode = 'max' instead.\n",
    "\n",
    "\n",
    "# Model 'Refitting'\n",
    "\n",
    "# model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test),batch_size=128, epochs=250, callbacks = [early_stop])\n",
    "# Do note that the callbacks has been added which was defined above\n",
    "\n",
    "# Reevaluating the model after Early Stopping\n",
    "\n",
    "# loss_df = pd.DataFrame(model.history.history)\n",
    "# loss_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks(Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After Performing the model compilation, define the scheduler and put it under callbacks when fitting the model\n",
    "## The scheduler says that once the epoch hits 50, the learning rate will be higher\n",
    "\n",
    "# def scheduler(epochs, lr):\n",
    "#   if epochs >= 50:\n",
    "#     return 0.0001\n",
    "#   return 0.001\n",
    "\n",
    "# scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test),batch_size=128, epochs=250, callbacks = [scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                   Adding Dropout Layers[Prevent Overfitting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# You can perform this step when making the first model or after evaluating it and you see that it was Overfitting\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Dense(4,activation='relu'))\n",
    "# model.add(Dropout(0.5))              ---> This will turn off 50% of the Neurons at random each time\n",
    "\n",
    "# model.add(Dense(2,activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(1,activation='relu'))       \n",
    "                                          \n",
    "# model.compile()                     ---> Don't copy and paste this command ah, follow the one on top.\n",
    "\n",
    "\n",
    "# model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test),batch_size=128, epochs=250, callbacks = [early_stop])\n",
    "# Notice that Early Stopping has also been included here to further prevent Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                            Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                               [For Regression Task]\n",
    "\n",
    "# model.evaluate(X_test,y_test,verbose=0) ----> This will return the Mean Squared Error of the losses\n",
    "#                                         ----> The model that we trained is now put to the test with values X_test & y_test \n",
    "#                                               which it has never seen before.\n",
    "#                                         ----> The lower the Mean Squared Error, the better the model.\n",
    "# model.predict(X_test) ----> This will output an array of the model's predictions of y(Price in this case) using only\n",
    "#                             the data presented in X_test.\n",
    "\n",
    "# test_predictions = model.predict(X_test) \n",
    "# test_predictions = pd.Series(test_predictions.reshape(300,)) ----> We can then make it into a Panda series to compare it\n",
    "#                                                                    side by side with the actual y(Price) figure.\n",
    "#                                                              ----> The 300 just represented 300 rows with 1 column for this case\n",
    "# pred_df = pd.DataFrame(y_test, columns=['Test True Y']) ----> This is to map the true y(Price) value to be concatenate \n",
    "#                                                               with the predicted y to see if the figures are close.\n",
    "# pred_df = pd.concat([pred_df, test_predictions], axis=1) ----> Mapping the columns together\n",
    "# pred_df.columns = ['Test True Y','Model Predictions'] ----> Naming the columns\n",
    "# sns.scatterplot(x='Model Predictions', y= 'Test True Y', data=pred_df) ----> Creating a scatterplot for visualisation\n",
    "\n",
    "#                       Mean Absolute Error / Mean Squared error / Explained Variance Score\n",
    "\n",
    "\n",
    "# Importing\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# mean_squared_error(pred_df['Test True Y'], pred_df['Model Predictions']) ----> This will output the mean squared error.\n",
    "#                                                                          ----> Do note that the first parameter must be the true value\n",
    "\n",
    "\n",
    "# explained_variance_score(y_test, test_predictions) ---> Best score is 1.0.\n",
    "#                                                    ---> It shows how much variance is explained by the model\n",
    "\n",
    "\n",
    "#                                             [For Classification Task]\n",
    "\n",
    "# model.predict_classes(X_test) ---> Shows the classisfications based on the X_test dataset.\n",
    "# predictions = model.predict_classes(X_test)\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# print(classification_report(y_test, predictions)) ---> This report takes in the true value of y and compare it with Predictions\n",
    "\n",
    "# print(confusion_matrix(y_test, predictions)) ---> To check on the performance of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                          Predicting a 'New' item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input_a_name = df.drop('Column to be predicted', axis = 1).iloc[0] ---> This process is purely to check on the existing set. \n",
    "#                                                                         If it is a real data, don't do this\n",
    "\n",
    "# inputted_name = inputted_name.values.reshape(-1, Number of features)  ---> Converts to array. \n",
    "#                                                                            The -1 means keep old dimensions along axis\n",
    "\n",
    "# inputted_name = scaler.transform(inputted_name) ---> To scale it\n",
    "\n",
    "# model.predict(inputted_name) ---> Returns the prediction done by the model Eg; Price of a house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                               Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# model.save('Input name.h5')                         ----> Need to put the h5 inside\n",
    "# model_to_load = load_model('The saved model's name) ----> This is to load the model, the model_to_load is just an example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
