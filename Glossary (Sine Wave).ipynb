{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                 Creating a Sine Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(0, 50, 501) ---> Creating an array of numbers from 0 to 50 with 501 numbers getting to it.\n",
    "# y = np.sin(x) ---> Making y the sine of x\n",
    "\n",
    "# plt.plot(x,y) ---> This will create a sine wave from 0 to 50 using the above numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                              Creating the Sine Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data = y, index = x, columns = ['Sine']) ---> This is provided you have established the variables x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                   Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Determine how much samples are used for testing. we will use 10%\n",
    "# test_percent = 0.1\n",
    "\n",
    "# Step 2: Making a test point to have 10% of the total sample size and rounding it off\n",
    "#         This method can be used to apply on any other samples in the future\n",
    "# test_point = np.round(len(df)*test_percent)\n",
    "\n",
    "# Step 3: Just minusing the total length of dataframe with the test_point of 10%\n",
    "# test_index = int(len(df) - test_point)\n",
    "\n",
    "# Step 4: Creating the training split. We use integer location slicing\n",
    "# train = df.iloc[:test_index] ---> This will take all the samples from the start to before the test_index\n",
    "\n",
    "# Step 5: Creating the training split. We use integer location slicing\n",
    "# test = df.iloc[test_index:] ---> This will take all the samples from test_index to end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                       Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scaling method will be done differently in RNN as they are going to feed output back into themselves\n",
    "# Therefore, we will have to scale our label(y) as well.\n",
    "# We will follow the rules of only fitting into our training dataset\n",
    "\n",
    "# from sklearn.preprocessieng import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(train)\n",
    "\n",
    "# scaled_train = scaler.transform(train)\n",
    "\n",
    "# scaled_test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         Creating Batches of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# length = 2 ---> Length of the output sequence. Do note that the larger the length, the longer time it will take to predict\n",
    "# batch_size = 1\n",
    "\n",
    "\n",
    "# generator = TimeseriesGenerator(scaled_training_data, scaled_training_data, length = length, batch_size = batch_size)\n",
    "\n",
    "# [Do note that the first parameter is the index which is x and the second parameter is the label which is y.] \n",
    "# [Since they are both present in the training data, we use it for both in this case]\n",
    "# [The generator will generate batches for us]\n",
    "\n",
    "# X,y = generator[0] ---> Returns both the x and y from the generator. This is the first batch generated.\n",
    "\n",
    "# If we output X itself, it will have a 2 arrays if we set the length as 2.\n",
    "# If we output Y itself, it will only have 1.\n",
    "# What we are essentially telling the RNN is that given the length(parameter in generator) of the arrays in X, \n",
    "# predict the y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                               Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "\n",
    "# n_features = 1 ---> Basically x trying to predict y. So number of features should be 1\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(SimpleRNN(50, input_shape=(length, n_features)))\n",
    "# [The 50 is the number of neurons you want. in this case, since we have length of 50, we put 50.]\n",
    "# [The input_shape is the length(50) by the number of features(1) which is essentially 50 rows and 1 column according to example]\n",
    "# [You can also put in the numbers manually eg; input_shape(50,1)]\n",
    "\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.summary() ---> Shows the different layers\n",
    "\n",
    "# model.fit_generator(generator, epochs=5)\n",
    "# [The fit_generator is from the library. The generator is the variable we declared above]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Loss Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = pd.DataFrame(model.history.history)\n",
    "\n",
    "# losses.plot() ---> Plots the loss\n",
    "\n",
    "\n",
    "#                                          Evaluate the Test Data\n",
    "# Essentially what we are doing is taking the last 50 points from training set and predicting the first point \n",
    "# into the future in the test set.\n",
    "\n",
    "# first_evaluate_batch = scaled_train[-length:] ---> Do note that the length was already declared above\n",
    "#                                               ---> You put the -length before the : is to take the last 50.\n",
    "#                                               ---> If you put the it like [:-length] it will take the everything except\n",
    "#                                                    the last 50\n",
    "\n",
    "# first_evaluate_batch= first_evaluate_batch.reshape((1,length, n_features))\n",
    "\n",
    "# model.predict(first_evaluate_batch) ---> Outputs the prediction on the first point in the future.\n",
    "# scaled_test[0] ---> Shows the very first point of the test set to see if the prediction is close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                             Creating the For-Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do note that the purpose for creating this For-Loop is the predict the ALL tests results by taking the initial 50 datas from\n",
    "# training dataset and then predicting the the next point after the training which is the first point of test and repeating  \n",
    "# this steps whereby now we take 50(49 training and 1 predicted result) samples to predict the second point of test \n",
    "# and so on and forth.\n",
    "\n",
    "# Step 1: Creating an empty array to store the predicted test results\n",
    "# test_predictions = []\n",
    "\n",
    "# Step 2: Taking the last 50 datasets from scaled training data as the first evaluation batch\n",
    "# first_evaluation_batch = scaled_train[-length:]\n",
    "\n",
    "# Step 3: Reshaping the batch to fit the For-Loop\n",
    "# current_batch= first_evaluate_batch.reshape((1,length, n_features))\n",
    "\n",
    "#============================================[DO NOT USE THIS CODE]==========================================================\n",
    "\n",
    "# np.append(current_batch[:,1:,:],[[[99]]], axis=1) ---> This code is essentially saying to move the current_batch along \n",
    "#                                                        by one time step. We are using this for the for loop below.\n",
    "\n",
    "#============================================================================================================================ \n",
    "\n",
    "# Step 4: Using the For-Loop to append the predicted results for test to test_predictions.\n",
    "# for i in range(len(test)):\n",
    "\n",
    "#    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "#    test_predictions.append(current_pred)\n",
    "    \n",
    "#    current_batch = np.append(current_batch[:,1:,:],[[current_pred]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            Inverse the Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_predictions = scaler.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                Plotting the Test data against the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['Predictions'] = true_predictions ---> Creating another column under the test variable which was established from train\n",
    "#                                             test split earlier to map the Predicted values by the model for comparison\n",
    "\n",
    "# test.plot() ---> Plotting the Actual tests against the predicted tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                           Include Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience = 2)\n",
    "\n",
    "\n",
    "#==========================================Only if we include Validation Data==================================================\n",
    "\n",
    "# If we create validation set, the length MUST be at least ONE shorter than the test data\n",
    "\n",
    "# [We have to redefine the length as it can't be the same size as the scaled test data]\n",
    "# length = (length of the test data) - 1\n",
    "\n",
    "# generator = TimeseriesGenerator(scaled_train, scaled_train, length = length, batch_size=1)\n",
    "\n",
    "# validation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=length, batch_size = 1)\n",
    "# [Do note that the length was already established as 50 from the example above]\n",
    "\n",
    "#===============================================================================================================================\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(50, input_shape=(length, n_features)))\n",
    "#[Do note that the RNN has been replaced with LSTM]\n",
    "\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.summary() ---> Shows the different layers\n",
    "\n",
    "# model.fit_generator(generator, epochs=20, validation_data=validation_generator, callbacks = [early_stop])\n",
    "# [The fit_generator is from the library. The generator is the variable we declared above]\n",
    "\n",
    "\n",
    "# Afterwards, you can proceed to do the For-Loop above with the exact same steps to do evaluate the test predictions\n",
    "# with the plotting etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                 Forecasting into the future[Scaling ALL the Data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially retraining all of the data\n",
    "# Therefore, we will be scaling all of the data\n",
    "\n",
    "# full_scaler = MinMaxScaler()\n",
    "# scaled_full_data = full_scaler.fit_transform(df) ---> Scaling all of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                             Forecasting into the future[Creating and fitting the model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = TimeseriesGenerator(scaled_full_data, scaled_full_data, length = length, batch_size = batch_size)\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(50, input_shape=(length, n_features)))\n",
    "# [Do note that the RNN has been replaced with LSTM]\n",
    "# [If we are satisfied with the LSTM model instead of RNN, we use LSTM]\n",
    "\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.fit_generator(generator, epochs = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                               Forecasting into the future[Forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast = []\n",
    "# first_evaluate_batch = scaled_train[-length:]\n",
    "# current_batch= first_evaluate_batch.reshape((1,length, n_features))\n",
    "\n",
    "# for i in range(25):\n",
    "\n",
    "#    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "#    forecast.append(current_pred)\n",
    "    \n",
    "#    current_batch = np.append(current_batch[:,1:,:],[[current_pred]], axis=1)\n",
    "\n",
    "# [Do note that instead of putting len(test) in the for loop, we get to decide how many spaces we are \n",
    "# going to forecast into the future. We use 25 spaces in the example above]\n",
    "\n",
    "\n",
    "\n",
    "# forecast = scaler.inverse_transform(forecast) ---> Inverting back the forecast\n",
    "\n",
    "# forecast_index = np.arange(50.1,52.6, step=0.1) ---> Since for the above example, our index stops at 50 and we have forecasted\n",
    "#                                                      25 steps into the future, this code mainly creates a new index starting \n",
    "#                                                      50.1 all through 52.6 for the forecasted figures to fit into it\n",
    "#                                                      to be plotted as shown in the code below.\n",
    "\n",
    "# plt.plot(df.index,df['Sine'])     ---> These 2 codes are to plot both the datas into a single table.\n",
    "# plt.plot(forecast_index,forecast)      Using this Sine example, we are essentially creating another new Sine wave 25 points \n",
    "#                                        into the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
