{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will only be selecting the 0s from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying one of the 2D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are doing this to get a smaller dataset for faster training and see if the generator is working or not\n",
    "# For this example, we will only be selecting Zeros.\n",
    "\n",
    "# only_zeros = X_train[y_train==0] ---> Grabs only zero images.\n",
    "# If you output y_train==0, it will output an array of booleans saying if it is True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do note that you can create the Discriminator or Generator first\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "# discriminator = Sequential()\n",
    "# discriminator.add(Flatten(input_shape=[28,28])) ---> Need to flatten the image as it is 2D image\n",
    "# discriminator.add(Dense(150, activation='relu')) ---> Just an arbitrary value(150)\n",
    "# discriminator.add(Dense(100, activation='relu')) ---> Do note that we can add in more layers to make discriminator  \n",
    "#                                                       harder to fool.\n",
    "# discriminator.add(Dense(1, activation='sigmoid')) ---> Regardless of how many digits you're working with, the discriminator \n",
    "#                                                        is just a binary     classification to see if the image is real \n",
    "#                                                        or fake. So it will only output ONE neuron\n",
    "\n",
    "# discriminator.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the Encoder topic, the Generator acts like a Decoder.\n",
    "# It takes some sort of encoding and expand it back out to generate some sort of output to pass through the discriminator\n",
    "# and see if it fools the discriminator\n",
    "\n",
    "# Eg; 784 --> 150 --> 30 --> 150 --> 784  [Neurons]\n",
    "#    =============    ===================\n",
    "#      Encoder       Decoder, Generator for this case\n",
    "\n",
    "\n",
    "# To start off, We will choose a coding size which is substantially less than 784 features or pixels BUT not too small(eg; 30) \n",
    "# that you have nothing to learn off from. \n",
    "# This 100 represents the 30 from the example on top.\n",
    "\n",
    "# codings_size = 100\n",
    "\n",
    "# generator = Sequential()\n",
    "# generator.add(Dense(100, activation='relu', input_shape=[codings_size])) ---> Do note that we can change the 100 at the start\n",
    "# generator.add(Dense(150, activation='relu'))\n",
    "# generator.add(Dense(784, activation='relu'))\n",
    "# generator.add(Reshape([28,28])) ---> Since our discrimator expects a 28 by 28 2D input, we need to reshape the output to match\n",
    "#                                      what the discrimator expects.\n",
    "\n",
    "# [Do note that we do not compile the generator. It is only trained through the GAN model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN = Sequential([generator, discriminator])\n",
    "\n",
    "# discriminator.trainable = False ---> .trainable is an attribute itself. We need to set it to false as it should not be trained\n",
    "#                                      during the second phase\n",
    "\n",
    "# GAN.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32 ---> Larger batch will have faster training time\n",
    "\n",
    "# my_data = only_zeros ---> We set up my_data so that it could optionally run like my_data=X_train\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(my_data).shuffle(buffer_size=1000)\n",
    "# [We are just shuffling in case it's in order. But best is do so]\n",
    "# buffer_size: We have to so that it doesn't load all the zero datas at once but shuffle in batches of 1000.\n",
    "\n",
    "# type(dataset)---> To confirm on the type of dataset\n",
    "\n",
    "# dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "# We have a remainder of zero samples(5923/32=185.094), we will remove the 0.94 to feed in 185 batches of 32\n",
    "# prefetch: This allows later elements to be prepared while the current element is being processed. \n",
    "#           This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "\n",
    "# epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN.layers ---> Gets back a list of Sequential models. \n",
    "#                 For the example here, there will be two models. (discriminator & generator)\n",
    "\n",
    "# GAN.layers[0].layers ---> Returns the layers of the first Sequential model. (Discriminator)\n",
    "# GAN.layers[1].layers ---> Returns the layers of the second Sequential model. (Generator)\n",
    "\n",
    "# GAN.layers[0].summary() ---> Returns the summary of the first Sequential model.\n",
    "# GAN.layers[1].summary() ---> Returns the summary of the second Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### DO NOT USE THIS CODE!! JUST FOR REFERENCE!! USE THE ONE BELOW THIS CHUNK#########################\n",
    "\n",
    "# generator, discriminator = GAN.layers ---> We will grab the seperate components before starting the loop\n",
    "# epochs = 1\n",
    "# [We usually fit the model. But since this is two distinct phases]\n",
    "\n",
    "# for epoch in range(epochs):                 # Just training for 1 epoch since the epochs declared above is 1.\n",
    "#     print(f\"Currently on Epoch {epoch+1}\")  # Printing message to make sure we're running\n",
    "#     i = 0                                   # i=0 as we are only running once\n",
    "#     # For every batch in the dataset         For every batch in dataset,\n",
    "#     for X_batch in dataset:\n",
    "#         i=i+1\n",
    "#         if i%100 == 0:                      # After completing 100 batches, we will print out the below to track it.\n",
    "#             print(f\"\\tCurrently on batch number {i} of {len(my_data)//batch_size}\")\n",
    "\n",
    "\n",
    "\n",
    "# Discriminator Training\n",
    "\n",
    "#          noise = tf.random.normal(shape = [batch_size,codings_size]) ---> Creating noise\n",
    "\n",
    "#          gen_images = generator(noise) ---> All the generator sees are the random noise declared on top.\n",
    "#                                             Based off the model shape declared on top(the layers and all), can it produce an image\n",
    "#                                             Do note that it WILL NOT produce any types of clear image as they don't even know it.\n",
    "\n",
    "#          X_fake_vs_real = tf.concat([gen_images, tf.dtypes.cast(X_batch,tf.float32)], axis=0)\n",
    "# [This will concatenate generated images with the real ones]\n",
    "# [TO use tf.concat, the data types must match!]\n",
    "\n",
    "\n",
    "#          y1 = tf.constant([[0.0]]*batch_size + [[1.0]]*batch_size)\n",
    "# [This will set the target labels for the discriminator]\n",
    "# [The [[0.0]] is a list so if multiply by 10, it will give 10 [[0.0]]s.]\n",
    "#[ We are just concatenating with [[1.0]]*batch_size as the 0s are the false generated images(noise) \n",
    "#  and the 1s are actual images]\n",
    "\n",
    "#          discriminator.trainable = True   #Discriminator is false only during GAN compiling\n",
    "\n",
    "#          discriminator.train_on_batch(X_fake_vs_real,y1)\n",
    "\n",
    "\n",
    "\n",
    "# Generator Training\n",
    "# [The generator will start to produce fake images and the discriminator will try again to guess if the images are real or fake]\n",
    "\n",
    "#          noise = tf.random.normal(shape=[batch_size,codings_size])  # We create random noise again\n",
    "#          y2 = tf.constant([[1.0]]*bastch_size)   # We want the discriminator to believe the fake images are real. So we set all to 1\n",
    "#          discriminator.trainable=False\n",
    "#          GAN.train_on_batch(noise,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 1\n",
    "# # We will grab the seperate components before starting the loop\n",
    "# generator, discriminator = GAN.layers\n",
    "\n",
    "# # For every epcoh\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"Currently on Epoch {epoch+1}\")\n",
    "#     i = 0\n",
    "#     # For every batch in the dataset\n",
    "#     for X_batch in dataset:\n",
    "#         i=i+1\n",
    "#         if i%100 == 0:  # After completing 100 batches, we will print out the below to track it.\n",
    "#             print(f\"\\tCurrently on batch number {i} of {len(my_data)//batch_size}\")\n",
    "#         #####################################\n",
    "#         ## TRAINING THE DISCRIMINATOR ######\n",
    "#         ###################################\n",
    "        \n",
    "#         # Create Noise\n",
    "#         noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "        \n",
    "#         # Generate numbers based just on noise input\n",
    "#         # All the generator sees are the random noise declared on top.\n",
    "#         # Based off the model shape declared on top(the layers and all), can it produce an image\n",
    "#         # Do note that it WILL NOT produce any types of clear image as they don't even know it.\n",
    "#         gen_images = generator(noise)\n",
    "        \n",
    "#         # Concatenate Generated Images against the Real Ones\n",
    "#         # TO use tf.concat, the data types must match!\n",
    "#         X_fake_vs_real = tf.concat([gen_images, tf.dtypes.cast(X_batch,tf.float32)], axis=0)\n",
    "        \n",
    "#         # Targets set to zero for fake images and 1 for real images\n",
    "#         # This will set the target labels for the discriminator\n",
    "#         # The [[0.0]] is a list so if multiply by 10, it will give 10 [[0.0]]s.\n",
    "#         # We are just concatenating with [[1.0]]*batch_size as the 0s are the false generated images(noise) \n",
    "#         # and the 1s are actual images]\n",
    "#         y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "        \n",
    "#         # This gets rid of a Keras warning\n",
    "#         discriminator.trainable = True\n",
    "        \n",
    "#         # Train the discriminator on this batch\n",
    "#         discriminator.train_on_batch(X_fake_vs_real, y1)\n",
    "        \n",
    "        \n",
    "#         #####################################\n",
    "#         ## TRAINING THE GENERATOR     ######\n",
    "#         ###################################\n",
    "#         # The generator will start to produce fake images and the discriminator will try again to guess if \n",
    "#         # the images are real or fake\n",
    "        \n",
    "#         # Create some noise\n",
    "#         noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "        \n",
    "#         # We want discriminator to belive that fake images are real\n",
    "#         y2 = tf.constant([[1.]] * batch_size)\n",
    "        \n",
    "#         # Avoids a warning\n",
    "#         discriminator.trainable = False\n",
    "        \n",
    "#         GAN.train_on_batch(noise, y2)\n",
    "        \n",
    "# print(\"TRAINING COMPLETE\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ask for some 'noise'\n",
    "\n",
    "# noise = tf.random.normal(shape=[10,codings_size])  # We are just requesting for 10 fake images by the coding size.\n",
    "# plt.imshow(noise) # This just shows the noise. Do note that the x axis is the coding size and y is the 10.\n",
    "\n",
    "# Step 2: Pass it into the generator to get some images\n",
    "\n",
    "# images = generator(noise)\n",
    "# images.shape  # Notice the shape of it [10,28,28]. It is saying 10 images of 28 by 28 pixels\n",
    "\n",
    "# Step 3: Showing the first image\n",
    "# images[0]  # Displays the arrays of the first shape\n",
    "# plt.imshow(images[0])  # Display the first image itself.\n",
    "\n",
    "# Do note that there may be an instance of 'Mode Collapse' which is the common issue in GAN whereby the generator has figured \n",
    "# out a way to fool the discriminator. So, it would 'think' why bother learn other types of images to fool the discriminator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
