{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a data which you can see that there is a yearly trend, we should really perform a train test split for\n",
    "# at least a year's information. Maybe 1.5 years?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                              Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size = 18 ---> Essentially taking 1.5 years in terms of months\n",
    "#                     This is provided that the data given is in months\n",
    "\n",
    "# test_index = len(df) - test_size\n",
    "\n",
    "# train = df.iloc[:test_index] ---> Essentially taking all the datas from beginning till before the test index as training data\n",
    "# test = df.iloc[test_index:] ---> Essentially taking all the datas from test index all the way till the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                             Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# scaler.fit(train) ---> Just as before, we do not want to assume that we have any information about the future dataset.\n",
    "#                        Therefore, we are only scaling our training data.\n",
    "\n",
    "# scaled_train = scaler.transform(train)\n",
    "# scaled_test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                             Time Series Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Do note that since we are adding EarlyStopping and Validation Generator, the length of the batch MUST be lesser than the test\n",
    "# set.\n",
    "\n",
    "# length = 12 ---> Again, we are putting 12 to represent 12mths should the data provided be in months not days etc\n",
    "\n",
    "# generator = TimeseriesGenerator(scaled_train,scaled_train, length = length, batch_size=1)\n",
    "`\n",
    "# X,y = generator[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                     Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM\n",
    "# from tensorflow.keras.callbacks import EarlyStopping ---> If we wanna include Earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = 1\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(100, activation = 'relu', input_shape=(length, n_features))) ---> the 100 represents the number of neurons\n",
    "# [Do note that we did not include activation in the previous Sine wave example as we were using the default activation \n",
    "# function. So here we are selecting Rectified Linear Unit.]\n",
    "\n",
    "# model.add(Dense(1))\n",
    "# model.compile(optimizer = 'adam', loss='mse')\n",
    "\n",
    "# model.summary() ---> Shows our Sequential model\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience = 2) ---> Best practice to have more patience than 1 for RNN\n",
    "\n",
    "# validation_generator = TimeseriesGenerator(scaled_test,scaled_test, length = length, batch_size=1)\n",
    "#[Since we are also fitting the validation data into the training, the validation data MUST also be a generator]\n",
    "\n",
    "# model.fit_generator(generator, epochs = 20, validation_data=validation_generator, callbacks=[early_stop])\n",
    "# [The fit_generator is from the library. The generator is the variable we declared above]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                             Visualising the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = pd.DataFrame(model.history.history)\n",
    "\n",
    "# losses.plot() ---> Plots the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                           Evaluate the Test Data[For-Loop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do note that the purpose for creating this For-Loop is the predict the ALL tests results by taking the initial 12 datas from\n",
    "# training dataset and then predicting the the next point after the training which is the first point of test and repeating  \n",
    "# this steps whereby now we take 12(11 training and 1 predicted result) samples to predict the second point of test \n",
    "# and so on and forth.\n",
    "\n",
    "# Step 1: Creating an empty array to store the predicted test results\n",
    "# test_predictions = []\n",
    "\n",
    "# Step 2: Taking the last 100 datasets from scaled training data as the first evaluation batch\n",
    "# first_evaluation_batch = scaled_train[-length:]\n",
    "\n",
    "# Step 3: Reshaping the batch to fit the For-Loop. We are passing in just 1 item per batch which is why we put 1 in front\n",
    "# current_batch= first_evaluate_batch.reshape((1,length, n_features))\n",
    "\n",
    "#============================================[DO NOT USE THIS CODE]==========================================================\n",
    "\n",
    "# np.append(current_batch[:,1:,:],[[[99]]], axis=1) ---> This code is essentially saying to move the current_batch along \n",
    "#                                                        by one time step. We are using this for the for loop below.\n",
    "\n",
    "#============================================================================================================================ \n",
    "\n",
    "# Step 4: Using the For-Loop to append the predicted results for test to test_predictions.\n",
    "# for i in range(len(test)):\n",
    "\n",
    "#    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "#    test_predictions.append(current_pred)\n",
    "    \n",
    "#    current_batch = np.append(current_batch[:,1:,:],[[current_pred]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                Inversing the Scaled Data & Plotting against Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# test['Predictions'] = true_predictions ---> Creating another column under the test variable which was established from train\n",
    "#                                             test split earlier to map the Predicted values by the model for comparison\n",
    "\n",
    "# test.plot() ---> Plotting the Actual tests against the predicted tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                          Forecasting into the Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially retraining all of the data\n",
    "# Therefore, we will be scaling all of the data\n",
    "\n",
    "# full_scaler = MinMaxScaler()\n",
    "\n",
    "# scaled_full_data = full_scaler.fit_transform(df) ---> Scaling all of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                            Model Creation[Forecasting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = 12 ---> The 12 represents the 12 months. Please please don't use this for all other datas\n",
    "\n",
    "# generator = TimeseriesGenerator(scaled_full_data, scaled_full_data, length = length, batch_size = 1)\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(100, activation = 'relu', input_shape=(length, n_features)))\n",
    "# [Do note that the RNN has been replaced with LSTM]\n",
    "# [If we are satisfied with the LSTM model instead of RNN, we use LSTM]\n",
    "\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.fit_generator(generator, epochs = 8)\n",
    "\n",
    "# [For the amount of epochs do decide, we can use the one with the lowest loss on the validation data from the graph we\n",
    "# did when training the data. So in this case, the loss was the lowest in epoch 8]\n",
    "# [Notice that we did not include the EarlyStop here. The reason is what is there to reference? We are forecasting leh, there\n",
    "# are no validation data for use to check against since we are using the full dataset.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                           Forecast into the Actual Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast = []\n",
    "\n",
    "# periods = 12 ---> Do note that we did not have this in the Sine Wave Glossary. We can adjust this to be any months into the\n",
    "#                   future like 12, 18, 6 months up to you. Do note that the longer you forecast, the more noise you will have\n",
    "#                   as you are adding in more and more predictions from the ground truth. Best practice is to forecast the same\n",
    "#                   number of periods/length as the batch length\n",
    "\n",
    "# first_evaluate_batch = scaled_train[-length:]\n",
    "# current_batch= first_evaluate_batch.reshape((1,length, n_features))\n",
    "\n",
    "# for i in range(periods):\n",
    "\n",
    "#    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "#    forecast.append(current_pred)\n",
    "    \n",
    "#    current_batch = np.append(current_batch[:,1:,:],[[current_pred]], axis=1)\n",
    "\n",
    "# [Do note that instead of putting len(test) in the for loop, we get to decide how many spaces we are \n",
    "# going to forecast into the future. We use 12 spaces in the example above]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                       Creating the Forecast Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast = scaler.inverse_transform(forecast) ---> Inverting back the forecast\n",
    "\n",
    "# forecast_index = pd.date_range(start='2019-11-01', periods=periods, freq=\"MS\")\n",
    "# start: The start is when you want the range to start at. For the example above, since the data we got is up till 2019-10-01,\n",
    "#        we will start from 2019-11-01 with the predicted values.\n",
    "# periods: This is the amount of periods you want to have. We put periods as it has already been established on top as 12.\n",
    "#          You can manually put inside 12 also.\n",
    "# freq: Its the frequency you want like in months, days etc. You can check the strings to use specifically by googling\n",
    "#       'pandas frequency strings'. For this case, we have Month Start(MS) frequency.\n",
    "\n",
    "\n",
    "# forecast_df = pd.DataFrame(data=forecast,index=forecast_index,columns=['Forecast'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                        Plotting the Forecasted Dataframe and the Dataframe itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting seperately\n",
    "\n",
    "# df.plot() ---> Original Data\n",
    "# forecast_df.plot() ---> Forecasted Data\n",
    "\n",
    "\n",
    "# Combining into the same plot\n",
    "# ax = df.plot() ---> Do note that the ax is short form for axis used for the parameter below\n",
    "# forecast_df.plot(ax=ax)\n",
    "# plt.xlim('2018-01-01', '2020-12-01') ---> Zooming in on the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
