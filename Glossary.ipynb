{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                        DISPLAYING DATAFRAME INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns ----> Displays all the Feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Feature1','Feature2']] ---> Displays ONLY the two features mentioned in the DataFrame. You can select more to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Column Name'].unique()   ----> Returns all the unique items in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Column Name'].nunique()   ----> Returns the TOTAL number of unique items in the column all added up into a single number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Column Name'].value_counts()  ----> Returns the amount of unique items present in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Column Name').sum() -----> Returns the sum of all the integers in other columns under the column \n",
    "                                        #eg; if groupby('Year').sum(), it will return all the years and passses of university\n",
    "    \n",
    "# df.groupby('Column Name').mean()['Column to show'] ---> Returns only the mean of the 'Column to show' grouped by\n",
    "#                                                         Column name\n",
    "\n",
    "# df.groupby('Column to be grouped')['Another Column's figures'].describe() ---> Returns the mean, std, count etc of the  \n",
    "#                                                                                'Another Column's figures' for each category \n",
    "#                                                                                grouped by the 'Column to be grouped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using max(), min(), idxmax(), idxmin()\n",
    "\n",
    "#df['Column name'].max() ---> Returns the highest integer in the column specified\n",
    "#df['Column name'].min() ---> Returns the lowest integer in the column specified\n",
    "#df['Column name'].idxmax() ---> Returns the highest integer INDEX LOCATION in the column specified\n",
    "#df['Column name'].idxmin() ---> Returns the highest integer INDEX LOCATION in the column specified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info() ---> To see if there are any null values right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum() ----> Returns the total number of nulls in each columns\n",
    "# df.isnull().sum()[df.isnull().sum() > 0] ---> Returns ONLY the columns with missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe().transpose() ----> Displays the count, mean, std ,min, 25%, 50% of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.corr() ----> Displays the correlation of all the items in the Dataframe among each other\n",
    "# df.corr()['Feature to check against all features'] ----> Displays only the correlation of the specified features agaisnt \n",
    "#                                                          the others\n",
    "\n",
    "#df.corr()['Feature to check against all features'].plot(kind='bar') --> Creates a bar graph for better visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying column types\n",
    "\n",
    "# df.select_dtypes(['object']).columns ---> Displays all the columns that are object type (non-numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Displaying the number of unique values in a column.] \n",
    "# [We refer to the number of unique entries of a categorical variable as the cardinality of that categorical variable.]\n",
    "\n",
    "# # Get number of unique entries in each column with categorical data\n",
    "# object_nunique = list(map(lambda col: df[col].nunique(), object_cols))\n",
    "# d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# # Print number of unique entries by column, in ascending order\n",
    "# sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                        COLUMN MANIPULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing data\n",
    "\n",
    "# df.fillna(value = 'Value you want to replace missing value') ---> This will replace it with a string as it is ''\n",
    "# df.fillna(value = Number you want to replace missing value) \n",
    "# df['Column to be applied] = df['Column to be applied'].fillna(value = 0) ---> Replaces all missing data in column with 0\n",
    "\n",
    "# Filling missing data in column\n",
    "# df['Column you want to replace'] = df['Column you want to replace'].fillna(value = input your value)\n",
    "\n",
    "# df['X'].fillna(value=df['X'].mean()) ---> Replaces missing value in the column with the average (Row A + Row C)/2\n",
    "#                                         [Do note that the number to be divided is only the ones which has number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing data\n",
    "\n",
    "# df.dropna() ---> Returns rows where nothing is missing\n",
    "# df.dropna(axis = 1) ---> Returns columns where nothing is missing\n",
    "# df.dropna(axis = 1, thresh=2) ---> Can only accept a maximum of 2 missing items in the dataframe(not the columns ah)\n",
    "\n",
    "\n",
    "# Dropping Categorical Data only\n",
    "# df.select_dtypes(exclude=['object'])\n",
    "\n",
    "\n",
    "# Dropping Rows with Empty data\n",
    "\n",
    "# df.dropna(axis = 0, subset=['Column with some empty data'], inplace = True) # Do note that the default axis is also 0.\n",
    "                                                                              # DO NOT MAKE IT df = this code!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only columns with the same categories in both Training and Validation Data.\n",
    "\n",
    "# # All categorical columns\n",
    "# object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# # Columns that can be safely label encoded\n",
    "# good_label_cols = [col for col in object_cols if \n",
    "#                    set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# # Problematic columns that will be dropped from the dataset\n",
    "# bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "# print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "# print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "\n",
    "# # Drop categorical columns that will not be encoded\n",
    "# label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "# label_X_valid = X_valid.drop(bad_label_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "# [Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each column.]\n",
    "# [The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than \n",
    "#  you would get from dropping the column entirely.]\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# my_imputer = SimpleImputer()\n",
    "# imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train)) \n",
    "# imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))    # DO NOTE THAT THE VALIDATION DATA CANNOT FIT AH\n",
    "# [The imputed_X_train/valid will be the datasets with filled NAs]\n",
    "\n",
    "# imputed_X_train.columns = X_train.columns\n",
    "# imputed_X_valid.columns = X_valid.columns\n",
    "# [Imputation removed column names; put them back]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Specified Datas from Features\n",
    "\n",
    "# specified_data = df[(df['Feature1'] == 'specified_data_you_want') | (df['Feature2'] == 'another_specified_data_you_want')]\n",
    "\n",
    "# This will create a dataframe with only the specified datas under the Feature.\n",
    "\n",
    "# [Do also note that the Feature1 can be used for Feature2 as well which will pull out the specified datas from just \n",
    "#  Feature1. ]g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['New Column'] = df['Column Name'] ----> Creates a new column at the end of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping a column\n",
    "\n",
    "# df.drop('Column to be removed', axis = 1) ----> Removes the column.\n",
    "# df = df.drop('Column to be removed', axis =1) ----> The removal will be applied to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop_duplicates()  ----> Removes all the duplicated items. \n",
    "                            #The items in the column and index must all be the same in order to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Items in the Column\n",
    "\n",
    "# df['Column Name']=df['Column Name'].replace(['Item1 to be replaced', 'Item2 to be replaced'], 'With this Item')\n",
    "# The line above will permanently replace the Items1 & 2 with this Item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Apply function.  [Do note that the Apply is apply when using it]\n",
    "\n",
    "# Define a function first. Eg\n",
    "# def get_first_letter(string without the ''):\n",
    "#    return string[0]\n",
    "# df['New Column'] = df['Column you want the function to be applied'].apply(get_first_letter)\n",
    "\n",
    "\n",
    "#Example 2\n",
    "\n",
    "# def get_washington_function(string without the ''):\n",
    "# if string[0] == 'W':\n",
    "#        return 'Washington'\n",
    "#    else:\n",
    "#        return 'Not Washington'\n",
    "    \n",
    "# df['New Column'] = df['Column you want the function to be applied'].apply(get_washington_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Map function [Do note that the Map is map when using it]\n",
    "\n",
    "# Define a map first\n",
    "\n",
    "# my_map = {'A':1,'B':2,'C':3}\n",
    "# df['Column to be mapped'].map(my_map)\n",
    "# This will replace all the letter A,B,C items in the column to 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dummy Variables\n",
    "\n",
    "# df['Column to replace with dummies'] = pd.get_dummies(df['Column to create'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns and replacing them [Do note that you must replace the exact number of columns]\n",
    "\n",
    "#df.columns ----> Shows all the columns in the data\n",
    "\n",
    "#To replace a total of 4 columns.\n",
    "#df.columns = ['New Column 1', 'New Column 2', 'New Column 3', 'New Column 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort Values\n",
    "\n",
    "#df.sort_values('Column to sort') ----> Returns the first alphabet from A on top to Z at bottom/ Lowest to Highest for integer\n",
    "#df.sort_values('Column to sort', ascending = False)  ---> Returns the last alphabet from Z on top and A at bottom \n",
    "#                                                         / Highest to lowest for integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Conversion\n",
    "\n",
    "# df['Column with date'] = pd.to_datetime(df['Column with date']) ----> Converts to date format\n",
    "#                                                           ----> Once converted, it is able to extract the Month,Date & Year\n",
    "\n",
    "# df['Year'] = df['date'].apply(lambda date: date.year) ----> This will create a new Year column \n",
    "#                                                      ----> Do note that the lambda basically acts like\n",
    "#                                                                         def year_extraction(date):\n",
    "#                                                                               return date.year\n",
    "# df['Month'] = df['date'].apply(lambda date: date.month)  ----> Same with year but month instead\n",
    "\n",
    "\n",
    "\n",
    "# Extracting only the year [Only for specific situations]\n",
    "\n",
    "# df['New Year Column'] = df['Year.column'].apply(lambda date:int(date[-4:]))\n",
    "\n",
    "# For the line above, it happens when you want to have a new Year column by extracting the dates fom column 'Year Column'\n",
    "# which assuming that it is formatted in DD-MM-YYYY and hence the lambda date: int(date[-4:]) which takes the last 4 digits\n",
    "# and converts them to integer.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dummies: It checks on the Mean Absolute Error etc with the datasets instantly\n",
    "# For more info: https://towardsdatascience.com/random-forest-and-its-implementation-71824ced454f\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# # Function for comparing different approaches\n",
    "# def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "#     model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     preds = model.predict(X_valid)\n",
    "#     return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "# print(\"MAE (Drop columns with missing values):\")\n",
    "# print(score_dataset(Your_X_train, Your_X_valid, y_train, y_valid)) ---> This will return your Mean absolute error of the data\n",
    "#                                                                         You've put into the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example if the feature has a categorical column like Grades(A,B,C,D), you will not be able to pass it into the model\n",
    "# as they can't read it. As such you will provide a dummy variable for each of the categories as shown below.\n",
    "# This will convert ALL of the categories under sub_grade into dummy variables.\n",
    "# DEPENDING ON SITUATION, you can then drop the initial sub_grade column and concat this new subgrade_dummies into the \n",
    "# dataframe.\n",
    "# Do note the drop_first is like if u have A,B,C liao if all are 0s, the last one(D) confirm will be 1.\n",
    "\n",
    "\n",
    "# subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)\n",
    "\n",
    "# df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1) ---> Dropping the initial column as it is useless now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding (Multiple Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Prerequisite (Dropping High cardinality(Lots of unique categories) columns.)\n",
    "\n",
    "## Step 1: Columns that will be one-hot encoded\n",
    "# low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "## Step 2: Columns that will be dropped from the dataset\n",
    "# high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## Step 1: Apply one-hot encoder to each column with categorical data\n",
    "# OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "# OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "# OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "## Step 2: One-hot encoding removed index; put it back\n",
    "# OH_cols_train.index = X_train.index\n",
    "# OH_cols_valid.index = X_valid.index\n",
    "\n",
    "## Step 3: Remove categorical columns (will replace with one-hot encoding)\n",
    "# num_X_train = X_train.drop(object_cols, axis=1)\n",
    "# num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "## Step 4: Add one-hot encoded columns to numerical features\n",
    "# OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "# OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "## Step 5: OPTIONAL (Random Forest Regressor) ---> You will have to copy paste and make necessary changes from Random Forest\n",
    "#                                                 Generator from the above code first\n",
    "# print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "# print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Features with many Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Taking the top 20 values sorted by the most number of times it is repeated and illustrating it\n",
    "\n",
    "#        Do note that the code below is just one of the few ways to do it.\n",
    "#        We will be taking ONLY the TOP 10 categories and converting them into dummy variable while removing the rest.\n",
    "\n",
    "#        df.[Insert column].value_counts().sort_values(ascending=False).head(20) \n",
    "\n",
    "\n",
    "#Step 2: Take in the top 10 categories of the intial column and setting it as a variable\n",
    "#        top_10 = [x for x in df.[Insert column].value_counts().sort_values(ascending=False).head(10).index]\n",
    "\n",
    "\n",
    "# Step 3: Looping it for Step 4.\n",
    "# for label in top_10:\n",
    "#    df[label] = np.where(df['Inserted column']==label,1,0) ---> If the initial column got the category, the new category column\n",
    "#                                                                will have 1 in the data. Else, it will be 0.\n",
    "\n",
    "# df[['Inserted column']+ top_10].head(40)\n",
    "\n",
    "\n",
    "# Step 4: Defining the Function \n",
    "\n",
    "#         def one_hot_top_x(df, variable, top_x_labels):\n",
    "#            for label in top_x_labels:\n",
    "#                df[variable+'_'+label] = np.where(data[varibale]==label,1,0)\n",
    "\n",
    "\n",
    "# Step 5: Applying the Function\n",
    "\n",
    "# one_hot_top_x(df, 'Inserted column', top_10)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Features with many Categories (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Columns that will be one-hot encoded\n",
    "# low_cardinality_cols = [col for col in object_cols if df[col].nunique() < 10]\n",
    "\n",
    "# # Columns that will be dropped from the dataset\n",
    "# high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "# print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "# print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: All categorical columns\n",
    "# object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Step 2: Get Columns that can be safely label encoded\n",
    "# good_label_cols = [col for col in object_cols if \n",
    "#                    set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Step 3: Get Problematic columns that will be dropped from the dataset\n",
    "# bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "# print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "# print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "\n",
    "# Step 4: Drop categorical columns that will not be encoded\n",
    "# label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "# label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Step 5: Apply label encoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# for col in good_label_cols:\n",
    "#     label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "#     label_X_valid[col] = label_encoder.fit_transform(X_valid[col])\n",
    "\n",
    "\n",
    "# Applying Label Encoder to Categories in the DataFrame.\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Step 1: All categorical columns\n",
    "# object_cols = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "      \n",
    "# print('Categorical columns that will be label encoded:', object_cols)\n",
    "\n",
    "# # Step 2: Apply label encoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# for col in object_cols:\n",
    "#     df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder (For combinations of strings and numbers in Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is to convert all the strings and floats etc within the feature to category and Label Encode it.\n",
    "\n",
    "# df[\"New Column with Encoding\"] = df[\"Column to Encode\"].astype('category')\n",
    "# df.dtypes # Shows the data types of all the DataFrame\n",
    "# df[\"New Column with Encoding\"] = df[\"New Column with Encoding\"].cat.codes\n",
    "\n",
    "\n",
    "# Alternatively, you can change all the categories within the Feature to either a Float or String first.\n",
    "\n",
    "# df['Column to convert to string'] = df.Column to convert.astype(str) ---> Converts the entire Feature data to strings\n",
    "# df['DataFrame Column'] = df['DataFrame Column'].astype(float) ---> DO NOTE that all the data inside MUST be of float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pipelines for Easier Referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the pipeline, we preprocess the training data and fit the model in a single line of code. \n",
    "# (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. \n",
    "# This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Preprocessing for numerical data\n",
    "# numerical_transformer = SimpleImputer(strategy='mean')  # Fills empty cells with the mean of the column. You can change this\n",
    "\n",
    "# # Preprocessing for categorical data\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),  # Selects the most frequent data\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))     # Ignores unknown categories and one hot encode it\n",
    "# ])\n",
    "\n",
    "# # Bundle preprocessing for numerical and categorical data. Do note that you don't need to have the EXACT number of columns\n",
    "# # as the X_test. Eg; if the categorical cols + numerical cols got lesser cols in total, no problem, the transformer will\n",
    "# # only be applied to the ones you specified below.\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, your_numerical_cols),\n",
    "#         ('cat', categorical_transformer, your_categorical_cols) \n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "# # Bundle preprocessing and modeling code in a pipeline\n",
    "# my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                               ('model', model)\n",
    "#                              ])\n",
    "\n",
    "# # Preprocessing of training data, fit model \n",
    "# my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Preprocessing of validation data, get predictions\n",
    "# preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# # Evaluate the model\n",
    "# score = mean_absolute_error(y_valid, preds)\n",
    "# print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                           DATAFRAME MANIPULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can read a table from html directly. Refer to the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([DataFrame1,DataFrame2], axis=1) ----> Returns a new dataframe with both dataframes added along the columns.\n",
    "#                                                 Do note that the rows must be the same.\n",
    "#                                                 The axis=1 is to ensure that the dataframes are mapped in line.\n",
    "#                                                 instead of mapping after all the rows of the first DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the numerical values from Dataframe\n",
    "\n",
    "# df = df.select_dtypes(exclude=['object'])   # This will remove all the columns with categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                             READING FILES:IMPORTING & EXPORTING THEM INTO THE NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd  ----> Returns the location of where your current notebook is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file\n",
    "\n",
    "# pd.read_csv('Filepath') ----> read in the file\n",
    "\n",
    "\n",
    "# Converting Dates in the file from object to date index when reading it\n",
    "\n",
    "# pd.read_csv('Filepath', parse_dates = True, index_col='DATE') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file\n",
    "\n",
    "# Lets say you did the pd.read_csv under df Eg\n",
    "# df = pd.read_csv('Filepath')\n",
    "# You can then\n",
    "# df.to_csv('output.csv') ---> It will be saved in the same folder as the notebook you are working on.\n",
    "#                               To save in another folder, you need to specify the path followed by output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# model = create_model(vocab_size,embed_dim,rnn_neurosn,batch_size=1)\n",
    "\n",
    "# model.load_weights('[Insert the model file here with the .h5]')\n",
    "\n",
    "# model.build(tf.TensorShape([1,None]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
